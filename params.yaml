data:
  train_start_date: "2015-01-01"
  train_end_date: "2017-08-15"
  test_start_date: "2017-08-16"
  test_end_date: "2017-08-31"
  min_train_date: "2017-03-01"  # Memory optimization: 16GB RAM limit with 90 features

features:
  lags: [16, 17, 18, 19, 20, 21, 28, 30, 60, 90, 180, 365]
  rolling_windows: [7, 14, 28, 60, 90, 180, 365]
  rolling_shift: 16
  rolling_stats: ["mean", "std", "min", "max"]
  use_target_encoding: true
  use_cyclical_encoding: true

train:
  model_type: "lightgbm"
  cv_splits: 4
  forecast_horizon: 16
  seed: 42

  lightgbm:
    objective: "tweedie"
    tweedie_variance_power: 1.5
    learning_rate: 0.02
    num_leaves: 255
    max_depth: -1
    min_child_samples: 50
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 0.1
    n_estimators: 3000
    early_stopping_rounds: 100
    n_jobs: -1
    device: "cpu"  # GPU bug with 90+ features; use CPU for full training

  xgboost:
    objective: "reg:tweedie"
    tweedie_variance_power: 1.5
    learning_rate: 0.02
    max_depth: 8
    min_child_weight: 50
    subsample: 0.8
    colsample_bytree: 0.8
    reg_alpha: 0.1
    reg_lambda: 0.1
    n_estimators: 3000
    early_stopping_rounds: 100
    tree_method: "hist"
    device: "cuda"

tuning:
  n_trials: 100
  timeout: null

ensemble:
  method: "weighted_average"
